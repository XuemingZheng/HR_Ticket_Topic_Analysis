{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc0ff00",
   "metadata": {},
   "source": [
    "# HR Ticket Topic Analysis using NLP and Clustering\n",
    "This notebook performs topic analysis on HR tickets using NLP and clustering techniques. It includes steps like preprocessing, vectorization, clustering, and topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd442a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================\n",
    "# 1. Import Dependencies\n",
    "# ==========================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# For Topic Modeling\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# For Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download NLTK data if needed\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8375d03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================\n",
    "# 2. Load and Inspect Data\n",
    "# ==========================\n",
    "\n",
    "# Replace 'hr_tickets_large.csv' with the path to your CSV file\n",
    "df = pd.read_csv('hr_tickets_large.csv')\n",
    "\n",
    "# Peek at the data\n",
    "print(\"Data Sample:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47eab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================\n",
    "# 3. Preprocess the Text\n",
    "# ==========================\n",
    "\n",
    "# Convert text to lowercase\n",
    "df['text_clean'] = df['Ticket_Text'].str.lower()\n",
    "\n",
    "# Remove punctuation using Regular Expression Tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['text_clean'] = df['text_clean'].apply(lambda x: ' '.join(tokenizer.tokenize(x)))\n",
    "\n",
    "# Remove stopwords and apply lemmatization\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['text_clean'] = df['text_clean'].apply(preprocess_text)\n",
    "\n",
    "print(\"Preprocessed Text Sample:\")\n",
    "df[['Ticket_Text', 'text_clean']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443720c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# 4. Vectorization using TF-IDF\n",
    "# ==============================================\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, min_df=2, ngram_range=(1,2))\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['text_clean'])\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5049939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# 5. Clustering with K-Means\n",
    "# ==============================================\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans_model.fit(tfidf_matrix)\n",
    "\n",
    "df['cluster'] = kmeans_model.labels_\n",
    "\n",
    "print(\"Cluster assignment counts:\")\n",
    "print(df['cluster'].value_counts())\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686615c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================================================\n",
    "# 6. Analyzing Top Terms per Cluster (Optional Insight)\n",
    "# ======================================================\n",
    "\n",
    "def get_top_keywords_for_cluster(tfidf_matrix, cluster_labels, vectorizer, top_n=10):\n",
    "    df_keywords = {}\n",
    "    for cluster_num in set(cluster_labels):\n",
    "        cluster_indices = [i for i, c in enumerate(cluster_labels) if c == cluster_num]\n",
    "        cluster_tfidf = tfidf_matrix[cluster_indices].mean(axis=0)\n",
    "        cluster_tfidf = np.asarray(cluster_tfidf).flatten()\n",
    "        top_indices = cluster_tfidf.argsort()[-top_n:][::-1]\n",
    "        top_features = [vectorizer.get_feature_names_out()[i] for i in top_indices]\n",
    "        top_scores = [cluster_tfidf[i] for i in top_indices]\n",
    "        df_keywords[cluster_num] = list(zip(top_features, top_scores))\n",
    "    return df_keywords\n",
    "\n",
    "top_keywords = get_top_keywords_for_cluster(tfidf_matrix, df['cluster'], tfidf_vectorizer, top_n=10)\n",
    "\n",
    "for cluster_num, keywords in top_keywords.items():\n",
    "    print(f\"\\nCluster {cluster_num} Top Keywords:\")\n",
    "    for word, score in keywords:\n",
    "        print(f\"{word}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9775f492",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================\n",
    "# 7. Topic Modeling with LDA\n",
    "# ==============================================\n",
    "\n",
    "num_topics = 5\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_model.fit(tfidf_matrix)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "n_top_words = 10\n",
    "\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_features_ind = topic.argsort()[:-n_top_words - 1:-1]\n",
    "    top_features = [feature_names[i] for i in top_features_ind]\n",
    "    print(f\"\\nTopic {topic_idx} top words:\")\n",
    "    print(\", \".join(top_features))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
